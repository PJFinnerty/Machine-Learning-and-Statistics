{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "## Glossary of Regression Terms\n",
    "\n",
    "### Coefficients <span style=\"color: blue;\">(*coef*)</span>\n",
    "\n",
    "In the regression of a dataset, the coefficients describe the statistical relationship between the independent variables and the dependent variables.\n",
    "\n",
    "The sign of the coefficent can tell you the direction of the relatioship between the variables. A positive sign identifies that as the independent variable increases, the mean of the dependent variable also increases, whilst a negative sign suggests a decrease.\n",
    "\n",
    "The value of the coefficient describes how the mean of the dependent variable changes in relation to the independent variable.\n",
    "\n",
    "Whilst carrying out regression on an indepedent variable and a dependent variable, it is important to hold the other variables 'constant'. That is to say it is imperative to study the effect of the independent variable on each dependent variable in isolation from the others (statisticsbyjim.com).\n",
    "\n",
    "The coefficients of the output are estimates of the actual data population, therefore, it is important to ensure that the model for regression follows best practice for that particular type of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "### Coefficient of Determination <span style=\"color: blue;\">*(R²)*</span>\n",
    "\n",
    "#### <span style=\"color: green;\">*High Percentage R² = Good fit; Low Percentage R² = Bad fit*</span>\n",
    "\n",
    "R² is the percentage of the response variable variation of a linear model. It measures how close the data are fitted by a line of regression (Frost, Statistics By Jim).\n",
    "\n",
    "R² is a valuable indicator for a Linear-regression model (including Polynomial models), however, it is important to check the test and training data of the model for signs of unwanted bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Standard Error of the Coefficient  <span style=\"color: blue;\">(*SE coef.* or *Std. Err.*)</span> and t-value  <span style=\"color: blue;\">(*t*)</span>\n",
    "\n",
    "#### <span style=\"color: green;\">*Std. Err: Low Values = Good fit; High Values = Bad fit*</span>\n",
    "\n",
    "#### <span style=\"color: brown;\">*t-value: High Values = Good fit; Low Values = Bad fit*</span>\n",
    "\n",
    "The Standard Error measures the accuracy of the unknown coefficient of the model. It is a floating point number that is always positive. The smaller the Std. Err. the more accurate the estimate is (Minitab.com). \n",
    "\n",
    "Dividing the coefficient by the standard error will produce a t-value or t-statistic. As a lower Std. Err. indicates lower error and the t-value calculates how many times the error divides into the coefficent itself, a larger t-value indicates greater accuracy. \n",
    "\n",
    "As a result, the t-value is essentially a measure of the error in relation to the coefficient.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Values  <span style=\"color: blue;\">(*p*)</span>, the Null Hypothesis <span style=\"color: blue;\">(*H0*)</span> and the Significance Level  <span style=\"color: blue;\">(*α* or *alpha level*)</span>   \n",
    "\n",
    "#### <span style=\"color: green;\">*Good Fit: when p-value* < *Significance Level*, and H0 is rejected </span>\n",
    "\n",
    "P-values work together with Coefficients to indicate the statistical value produced in the Regression process. Specifically, the P-values *of* the coefficients identify whether the relationships observed in a sample exist throughout the population (StatisticsByJim). \n",
    "\n",
    "The p-value is a number between 0 and 1.\n",
    "\n",
    "For each independent variable, there is a possibility that the variable has no correlation to the dependent variable, in which case there is not enough proof to display a relationship. This lack of a relationship is known as the Null Hypothesis and the P-values can test for this.\n",
    "\n",
    "If it is first necessary to reject the Null Hypothesis in order to determine that there is a significant enough effect between the variables in your sample in order to conclude that the same effect is present in the wider population. The significance level is the probability of dismissing the Null Hypothesis when it in fact is evident.\n",
    "\n",
    "The Significane Level, is a pre-determined threshold. It is normally set to a value of 0.05 (5%). However, the researcher must identify an appropriate threshold of Significance Level, from which to compare to the p-value. \n",
    "\n",
    "If the P-value is less than the significance level, the Null Hypothesis can be sufficiently rejected.\n",
    "\n",
    "As the coefficients, P-values and the Significance level are determined for each variable in isolation, this can determine what variables should be included in the Regression analysis. \n",
    "    \n",
    "The Significance Level for this project will be set at 5%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scikitlearn as sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "\n",
    "## Simple Linear Regression\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array, 'a', between 0 and 100, with each point at an interval of 5 and create a second array, 'b', with each point a multiple of 'a' and some 'noise' added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 20, 1)\n",
    "b = 3 * a + np.random.normal(0.0, 2, a.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 'a' and 'b' together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(a, b, 'k.')\n",
    "\n",
    "# Set some properties for the plot.\n",
    "plt.xlabel('Weight (KG)')\n",
    "plt.ylabel('Distance (CM)')\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Lines that roughly fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, b, 'k.', label=\"Data\")\n",
    "\n",
    "x = np.arange(0.0, 20.0, 2.0)\n",
    "plt.plot(x, 3.2 * x + 2.0, 'r-', label=r\"$3.2x + 2$\")\n",
    "plt.plot(x, 2.6 * x +  2.8, 'g-', label=r\"$2.6x + 2.8$\")\n",
    "plt.plot(x, 2.8 * x + 3.0, 'b-', label=r\"$2.8x + 3$\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Weight (KG)')\n",
    "plt.ylabel('Distance (CM)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "\n",
    "### Exponential and Linear Data\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 'd' and 'e' together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(0, 100, 1)\n",
    "e = np.linspace(1, 10, 100)\n",
    "f = 3 * e + np.random.exponential(3)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(d, e, 'k.')\n",
    "ax.plot(a, b, 'k.')\n",
    "\n",
    "exp = lambda d: 4**(d)\n",
    "log = lambda d: np.log(d)\n",
    "\n",
    "# Set y scale to exponential\n",
    "ax.set_yscale('function', functions=(exp, log))\n",
    "ax.set(xlim=(0,100), ylim=(0,4))\n",
    "ax.set_yticks([1, 3, 3.5, 3.75, 4.0])\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "d = np.arange(0, 100, 1)\n",
    "e = np.linspace(1, 10, 100)\n",
    "f = e + np.random.exponential(1, size = None)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(e, f, 'k.')\n",
    "\n",
    "exp = lambda d: 4**(d)\n",
    "log = lambda d: np.log(d)\n",
    "\n",
    "# Set y scale to exponential\n",
    "ax.set_yscale('function', functions=(exp, log))\n",
    "ax.set(xlim=(0,100), ylim=(0,4))\n",
    "ax.set_yticks([1, 3, 3.5, 3.75, 4.0])\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import curve fitting package from scipy\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the exponential with constants a and b\n",
    "def exponential(x, a, b):\n",
    "    return a*np.exp(b*x)\n",
    "\n",
    "# Generate dummy dataset\n",
    "x_dummy = np.linspace(start=5, stop=15, num=50)\n",
    "\n",
    "# Calculate y-values based on dummy x-values\n",
    "y_dummy = exponential(x_dummy, 0.5, 0.5)\n",
    "\n",
    "# Add noise from a Gaussian distribution\n",
    "noise = 5*np.random.normal(size=y_dummy.size)\n",
    "y_dummy = y_dummy + noise\n",
    "\n",
    "# Plot the noisy exponential data\n",
    "plt.scatter(x_dummy, y_dummy, s=20, color='#00b3b3', label='Data 2')\n",
    "plt.scatter(a, b, s=20, label='Data 1')\n",
    "\n",
    "#plt.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: Fit Exponential Distribution with Noise: https://stats.stackexchange.com/questions/151606/fit-exponential-distribution-with-noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: Stackoverflow - 'How can I exponentially scale the Y axis?': https://stackoverflow.com/questions/26198260/how-can-i-exponentially-scale-the-y-axis-with-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: Curve Fitting (Towards Data Science): https://towardsdatascience.com/basic-curve-fitting-of-scientific-data-with-python-9592244a2509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Library Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "#### Classification Definition (Indicative.com)\n",
    "\n",
    "Classification analysis is a data analysis task within data-mining, that identifies and assigns categories to a collection of data to allow for more accurate analysis. The classification method makes use of mathematical techniques such as decision trees, linear programming, neural network and statistics.\n",
    "\n",
    "Classification analysis can be used to question, make a decision, or predict behavior through the use of an algorithm. It works by developing a set of training data which contains a certain set of attributes as well as the likely outcome. The job of the classification algorithm is to discover how that set of attributes reaches its conclusion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: https://www.nltk.org/api/nltk.classify.scikitlearn.html \n",
    "\n",
    "#### Reference: https://www.indicative.com/resource/classification-analysis/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Regression\n",
    "\n",
    "#### Explained (Achemer.com):\n",
    "\n",
    "Regression analysis is a reliable method of identifying which variables have impact on a topic of interest. The process of performing a regression allows you to confidently determine which factors matter most, which factors can be ignored, and how these factors influence each other.\n",
    "\n",
    "In order to understand regression analysis fully, it’s essential to comprehend the following terms:\n",
    "\n",
    "Dependent Variable: This is the main factor that you’re trying to understand or predict. \n",
    "Independent Variables: These are the factors that you hypothesize have an impact on your dependent variable.\n",
    "\n",
    "#### Reference: alchemer.com/resources/blog/regression-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Clustering\n",
    "\n",
    "#### Qualtrics.com:\n",
    "\n",
    "Cluster analysis is a statistical method for processing data. It works by organising items into groups, or clusters, on the basis of how closely associated they are.\n",
    "\n",
    "Cluster analysis, like reduced space analysis (factor analysis), is concerned with data matrices in which the variables have not been partitioned beforehand into criterion versus predictor subsets. The objective of cluster analysis is to find similar groups of subjects, where “similarity” between each pair of subjects means some global measure over the whole set of characteristics.\n",
    "\n",
    "Cluster analysis is an unsupervised learning algorithm, meaning that you don’t know how many clusters exist in the data before running the model. Unlike many other statistical methods, cluster analysis is typically used when there is no assumption made about the likely relationships within the data. It provides information about where associations and patterns in data exist, but not what those might be or what they mean.\n",
    "\n",
    "In this article, we discuss various methods of clustering and the key role that distance plays as measures of the proximity of pairs of points.\n",
    "\n",
    "#### Reference: https://www.qualtrics.com/uk/experience-management/research/cluster-analysis/?rid=ip&prevsite=en&newsite=uk&geo=IE&geomatch=uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "##### Reference: https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/\n",
    "\n",
    "Dimensionality reduction refers to techniques for reducing the number of input variables in training data.\n",
    "\n",
    "When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data. This is called dimensionality reduction.\n",
    "\n",
    "— Page 11, Machine Learning: A Probabilistic Perspective, 2012.\n",
    "\n",
    "High-dimensionality might mean hundreds, thousands, or even millions of input variables.\n",
    "\n",
    "Fewer input dimensions often mean correspondingly fewer parameters or a simpler structure in the machine learning model, referred to as degrees of freedom. A model with too many degrees of freedom is likely to overfit the training dataset and therefore may not perform well on new data.\n",
    "\n",
    "It is desirable to have simple models that generalize well, and in turn, input data with few input variables. This is particularly true for linear models where the number of inputs and the degrees of freedom of the model are often closely related.\n",
    "\n",
    "The fundamental reason for the curse of dimensionality is that high-dimensional functions have the potential to be much more complicated than low-dimensional ones, and that those complications are harder to discern. The only way to beat the curse is to incorporate knowledge about the data that is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "A Gentle Introduction to Model Selection for Machine Learning\n",
    "by Jason Brownlee on December 2, 2019 in Machine Learning Process\n",
    "Tweet  Share\n",
    "Given easy-to-use machine learning libraries like scikit-learn and Keras, it is straightforward to fit many different machine learning models on a given predictive modeling dataset.\n",
    "\n",
    "The challenge of applied machine learning, therefore, becomes how to choose among a range of different models that you can use for your problem.\n",
    "\n",
    "Naively, you might believe that model performance is sufficient, but should you consider other concerns, such as how long the model takes to train or how easy it is to explain to project stakeholders. Their concerns become more pressing if a chosen model must be used operationally for months or years.\n",
    "\n",
    "Also, what are you choosing exactly: just the algorithm used to fit the model or the entire data preparation and model fitting pipeline?\n",
    "\n",
    "In this post, you will discover the challenge of model selection for machine learning.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "Model selection is the process of choosing one among many candidate models for a predictive modeling problem.\n",
    "There may be many competing concerns when performing model selection beyond model performance, such as complexity, maintainability, and available resources.\n",
    "The two main classes of model selection techniques are probabilistic measures and resampling methods.\n",
    "\n",
    "#### Reference: https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "#### Reference: https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d\n",
    "\n",
    "#### 1. Handling Null Values\n",
    "\n",
    "In any real-world dataset, there are always few null values. It doesn’t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Imputation\n",
    "\n",
    "Imputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Standardization\n",
    "\n",
    "It is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.\n",
    "\n",
    "##### Example:\n",
    "Consider the column Age from Dataframe 1. In order to standardize this column, we need to calculate the mean and standard deviation and then we will transform each value of age using the above formula.\n",
    "We don’t need to do this process manually as sklearn provides a function called StandardScaler.\n",
    "\n",
    "The important thing to note here is that we need to standardize both training and testing data.\n",
    "fit_transform is equivalent to using fit and then transform.\n",
    "\n",
    "fit function calculates the mean and standard deviation and the transform function actually standardizes the dataset and we can do this process in a single line of code using the fit_transform function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Handling Categorical Variables\n",
    "\n",
    "Handling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. Ex — color of an item is a discrete variable whereas its price is a continuous variable.\n",
    "\n",
    "##### Categorical variables are further divided into 2 types:\n",
    "\n",
    "**Ordinal categorical variables** — These variables can be ordered. Ex — Size of a T-shirt. We can say that M<L<XL.\n",
    "\n",
    "**Nominal categorical variables** — These variables can’t be ordered. Ex — Color of a T-shirt. We can’t say that Blue<Green as it doesn’t make any sense to compare the colors as they don’t have any relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Research on Wine Quality Dataset: material from https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "# Import train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Improt preprocessing module\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Import Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import cross-validation tools\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import metrics for model performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import module for storing arrays\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Load Dataset from file in Repository\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Remove Semicolons from the data\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('winequality-red.csv', sep=';')\n",
    "\n",
    "#print(data_2.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Check out the shape and summary statistics of the data\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    }
   ],
   "source": [
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data_2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Split the data: the 'quality' feature is the target feature and the \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_2.quality\n",
    "X = data_2.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Fit the transformer on the training set \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Transformer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying transformer to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.16664562e-16 -3.05550043e-17 -8.47206937e-17 -2.22218213e-17\n",
      "  2.22218213e-17 -6.38877362e-17 -4.16659149e-18 -2.54439854e-15\n",
      " -8.70817622e-16 -4.08325966e-16 -1.17220107e-15]\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    " \n",
    "print(X_train_scaled.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying transformer to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    " \n",
    "print(X_test_scaled.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02160495 1.00135689 0.97456598 0.91099054 0.86716698 0.94193125\n",
      " 1.03673213 1.03145119 0.95734849 0.83829505 1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Piplelne with preprocessing and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
