{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Classification\n",
    "Identifying which category an object belongs to\n",
    "E.g. Spam detection\n",
    "\n",
    "#### Regression\n",
    "Predicting an attribute associated with an object\n",
    "E.g. Stock prices prediction - what is a stock going to be worth tomorrow?\n",
    "E.g. weather\n",
    "\n",
    "#### Clustering\n",
    "Taking an automatic grouping of similar objects into sets\n",
    "E.g. Customer segmentation\n",
    "\n",
    "#### Model Selection\n",
    "Comparing, validating and choosing parameters and models.\n",
    "E.g. Imporving model accuracy via parameter tuning\n",
    "\n",
    "#### Dimesionality Reduction\n",
    "Reducing the number of random variables to consifer\n",
    "E.g. To increase model efficiency\n",
    "\n",
    "#### Pre-processing\n",
    "Feature extraction and normalisation\n",
    "E.g. Transforming input data such as text for use in machine learning algorithms\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit learn works on NUmpy arrays - homogenous arrays - have one datatype - different from DataFrames\n",
    "\n",
    "We assume that our input data is a matrix where Each row corresponds to one sample and each column corresponds to one feature\n",
    "\n",
    "In classification or regression, outputs or labels - different objects one for data and one for targets\n",
    "\n",
    "Most of the algorithms in sklearn assume that everything is a float - by default none of these work with categorical variables or missing values - this must be kept in mind\n",
    "\n",
    "### Supervised Machine Learning\n",
    "\n",
    "#### Machine learning pipeline:\n",
    "\n",
    "INSERT Supervised Machine Learning Image::\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "The above is an example is an object that encapsulates the algorithm for building the model and for making predictions - it will also store all the model parameters/\n",
    "\n",
    "If you call clf.fit() - this will build the model and store all the trees and the splits of the trees, from the RandomForestClassifier.\n",
    "\n",
    "All models have this fit() function and looks the same way - always get the training data (x_train), and if it's a supervised algorithm it also gets the outputs, y_train.\n",
    "\n",
    "If you apply the new data, you use clf.predict() on the object - you can use any new data you have, and it will return the predicions according to the model.\n",
    "\n",
    "There is also the score() function, which is a helper function that has both the prediction and it evaluates against some known truths - to provide information on the accuracy of the model.\n",
    "\n",
    "This is the most common interface for Sklearn and all models for regression and classification will follow it.\n",
    "\n",
    "Fit() and Predict() are the core methods, used on the object, clf.**\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "This is another interface in SKlearn, in unsupervised learning and preprocessing, where somebody gives you a trained dataset and you want to do a principle component analysis (PCA_.\n",
    "\n",
    "You have training data, you have no labels or known truths - you just have your matrix, 'x' and you build your model from this.\n",
    "\n",
    "Then when you get new data, or have your test data - you want to apply this model and it will give you a new view of the data.\n",
    "\n",
    "For example,a 'projection' onto the principle components. THis is a different task and it has a different interface...\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_new = pca.transform(X_Test)\n",
    "\n",
    "Everything is encapsulated as an object - we instantiate the pca object - we call fit() again, give it the training data, x, because it is an unsupervalised method. \n",
    "\n",
    "If we then want to project to the principle components, we use the transform() method on any data, this will give you the new view of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Basic API of Unsupervised Learning\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;estimator.fit(X, [y]) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | \n",
    "| :---| :--- | \n",
    "| **estimator.predict()** | **estimator.transform** | \n",
    "|  Classification | Presprocessing   | \n",
    "| Regression  | Dimensionality Reduction | \n",
    "| Clustering |Feature selection |    \n",
    "|    | Feature extraction   |   \n",
    "|   | \t   |\n",
    "| |  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above - estimators is a term for the model. It can refer to anything: like a classifier (e.g. RandomForest()) or a scaling method (e.g. StandardScaler()).\n",
    "\n",
    "When using the predict() method to make a prediction, estimators always take the data, 'x', and if it's a supervised mathod, it takes a target output, 'y'.\n",
    "\n",
    "We use the transform() method when you need a new view. For classification, regression or clustering, you use the transfer() method and if you want a new view, you use the transfer method on 'x' (new data). THis is used for preprocessing, dimensionality reduction, feature selection and feature extraction.\n",
    "\n",
    "This covers our two building blocks for unsupervised learning - transforming your data and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Model Evaluation and Selection\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of concepts to grasp within model selection and evaluation. Sklearn uses a number of methods to carry out these two processes.\n",
    "\n",
    "#### Two Examples:\n",
    "\n",
    "##### train_test_split\n",
    "When using train_test_splt - you use cross-validation. This is where you use 5 folds (5 splits) and this gives you a more robust estimate on the generalisation performance of your model.\n",
    "\n",
    "##### cross_val_score\n",
    "You can use cross_val_score() function to get an object, data and labels and input how much cross validation to use (3 fold, five fold etc). This function will return the scores on the 'holdut' set for each evaluation. For 5 folds, you will get 5 scores. You can then calculate the mean of the 5 scores.\n",
    "\n",
    "Iyt is important to remember that all models have parameters, and that you need to tune them. You take data, do train_test_split, you cross evaluate on the training data, in order to tune the parameters. You can then do a final evaluation of your test data. This gives you an unbiased evaluation of the generalisation performance.\n",
    "\n",
    "Just doing cross-validation to tune parameters, your estimate may be too 'optimistic'.\n",
    "\n",
    "##### Cross-validation Grid Search\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "params_grid = {'C':10, ** np.arange(-3, 3),\n",
    "                'gamma': 10, ** np.arange(-3,3)}\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.predict(X_test)\n",
    "grid.score(X_test, y_test)\n",
    "\n",
    "GridSearchCV implements GridSearch with cross-validation\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn doesn't automatically do any preprocessing because you won't have control over how to preprocess. Such as how to encode variables or how to compute data.\n",
    "\n",
    "More often than not, before you build your model from the training labels and training data, you are taking extra steps in between such as feature extraction, scaling and feature selection - when these steps have been implemented, they are included in the model.\n",
    "\n",
    "Cross-validation should happen on the whole processing pipeline, from feature extraction to feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/train_test_split.jpg\" style=\"width: 450px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipelines\n",
    "\n",
    "Similar to cross-validation, pipelines are a way to chain together transformations with the classifier. Making a Pipeline returns an object that is an estimator with the same interface as the other models.\n",
    "\n",
    "The pipeline therefore, looks like a model - except it includes some transformations with a classifier.\n",
    "\n",
    "Take for example, a pipeline that includes two transformations and one classifier. When .fit() is called on this model, it will transform using the first transformation, then fit the second transformation and then transform using the second transformation - it will then pass the transformed data onto the classifier.\n",
    "\n",
    "Using a preprocessing pipeline like this, will make it much more likely that you are not leaking information from your test set or that you're taking different actions on the test and training data. The pipeline encapsulates the data and standardises the processing of the data.\n",
    "\n",
    "Take scaling for example, the StandardScaler() function can be used inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is a simple and efficient tool for data mining and analysis.\n",
    "\n",
    "It is built on Numpy, SciPy and Matplotlib.\n",
    "\n",
    "It is an open source and commercially usable package.\n",
    "\n",
    "### What can we achieve using it?\n",
    "\n",
    "#### Classification\n",
    "Identifying which category an object belongs to\n",
    "E.g. Spam detection\n",
    "\n",
    "#### Regression\n",
    "Predicting an attribute associated with an object\n",
    "E.g. Stock prices prediction - what is a stock going to be worth tomorrow?\n",
    "E.g. weather\n",
    "\n",
    "#### Clustering\n",
    "Taking an automatic grouping of similar objects into sets\n",
    "E.g. Customer segmentation\n",
    "\n",
    "#### Model Selection\n",
    "Comparing, validating and choosing parameters and models.\n",
    "E.g. Imporving model accuracy via parameter tuning\n",
    "\n",
    "#### Dimesionality Reduction\n",
    "Reducing the number of random variables to consifer\n",
    "E.g. To increase model efficiency\n",
    "\n",
    "#### Pre-processing\n",
    "Feature extraction and normalisation\n",
    "E.g. Transforming input data such as text for use in machine learning algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Data Used |Coef. |R²| Std.Err.*(x1)* | p-value*(x1)*  | t-value*(x1)*    | H0 Rejected |    | \n",
    "| :---| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Simple Linear Regression**   |   |    |    |    |    |   |    |      |\n",
    "|  *polyfit() Method* |Full Data | [4.917, -13.899] | 72.9%| ✘| ✘|✘ |✘  | \n",
    "| *Sklearn Model*  | Train/Test | 4.866 |   74.6% |0.055  |0.00 |19.128| ✔| \n",
    "|  | |    |    |   |   |    |    |   |  \n",
    "|  **Polynomial Regression**   |    |    |    |   |   |    |    |   |  \n",
    "| *curve_fit() Method*  | Full Data\t   |[-0.0, 0.367. -6.771]  |91.1% |✘   |✘   | ✘   | ✘| \n",
    "| *Sklearn Model* |  Train Test | [-13.332,   1.937,  -0.053] | 88.1% | 0.019  | 0.00  |52.499   | ✔| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
